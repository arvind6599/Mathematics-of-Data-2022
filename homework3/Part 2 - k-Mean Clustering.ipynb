{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hands-on experiment 2: k-means Clustering by Semidefinite Programming\n",
    "\n",
    "Clustering is an unsupervised machine learning problem in which we try to partition a given data set into $k$ subsets based on distance between data points or similarity among them. The goal is to find $k$ centers and to assign each data point to one of the centers such that the sum of the square distances between them are minimal [1]. This problem is known to be NP-hard. \n",
    "\n",
    "### Clustering problem\n",
    "Given a set of $n$ points in a $d-$dimensional Euclidean space, denoted by\n",
    "\\begin{equation*}\n",
    "S = \\{ \\mathbf{s}_i = (s_{i1}, \\cdots, s_{id})^\\top~\\in \\mathbb{R}^d ~~ i = 1, \\cdots, n\\}\n",
    "\\end{equation*}\n",
    "find an assignment of the $n$ points into $k$ disjoint clusters $\\mathcal{S} = (S_1, \\cdots, S_k)$ whose centers are $\\mathbf{c}_j(j = 1, \\cdots, k)$ based on the total sum of squared Euclidean distances from each point $\\mathbf{s}_i$ to its assigned cluster centroid $\\mathbf{c}_i$, i.e.,\n",
    "\\begin{equation}\n",
    "f(S,\\mathcal{S}) = \\sum_{j=1}^k\\sum_{i=1}^{|S_j|}\\|\\mathbf{s}_i^{j} - \\mathbf{c}_j \\|^2,\n",
    "\\label{eq:kmeans}\\tag{Problem 1}\n",
    "\\end{equation}\n",
    "where $|S_j|$ is the number of points in $S_j$, and $\\mathbf{s}_i^{j} $ is the $i^{th}$ point in $S_j$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.part2.helpers import *\n",
    "from scipy.sparse import isspmatrix\n",
    "from lib.part2.Llyod_kmeans import *\n",
    "import sys\n",
    "sys.path.insert(0, \"lib/part2\")\n",
    "from plotter import plot_func, plot_comp\n",
    "# fix the seed\n",
    "random.seed( 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "Problem = sio.loadmat('lib/part2/data/clustering_data.mat')\n",
    "C = np.double(Problem['C']) # euclidean distance matrix\n",
    "N = int(Problem['N']) # number of data points\n",
    "k = Problem['k'] # number of clusters\n",
    "opt_val = Problem['opt_val'] # optimum value \n",
    "images = Problem['images'] # images for visualization after clustering\n",
    "labels = Problem['labels'] # true labels\n",
    "digits = Problem['digits'] # input data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2.1 Conditional gradient method for clustering fashion-mnist Data\n",
    "---\n",
    "#### <span style=\"font-variant:small-caps;\">(e) *(20 points)*</span>\n",
    "\n",
    "\n",
    " $\\triangleright$ Complete the missing lines in the function definitions of `HCGM` and `PDHG`, which implements Homotopy CGM and Vu-Condat algorithms, respectively. Run both methods $2000$ iterations to solve the $k$-means clustering problem.\n",
    "\n",
    "$\\triangleright$ Plot the convergence results of both algorithms using `plot_comp` function.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define operators\n",
    "We provide 2 operators and their conjugates:\n",
    "1. `A1`: Linear operator that takes the row sums\n",
    "2. `At2`: Conjugate of operator A1\n",
    "3. `A2`: Linear operator that takes the column sums \n",
    "4. `At2`: Conjugate of operator A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = lambda x: np.sum(x, axis = 1)\n",
    "At1 = lambda y: np.transpose(np.matlib.repmat(y, N, 1))\n",
    "A2 = lambda x: np.sum(x, axis = 0)\n",
    "At2 = lambda y: (np.matlib.repmat(y, N, 1))\n",
    "\n",
    "b = np.double(np.ones(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 1. Homotopy CGM\n",
    "\n",
    "**Remark:** For simplicity, there is only one penalty parameter $\\beta_k$ in the HCGM Algorithm. However, in practice, one can have different penalty parameters for different constraints. In our case, we advise you to ***multiply by 1000 the term $ (x_k - \\text{proj}_{\\mathcal{K}}(x_k))$** in Algorithm 1, in order to obtain a better practical convergence. This basically corresponds to having different penalty parameters for different constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HCGM(kappa=10, maxit=np.int(1e3), beta0=1):\n",
    "    # Initialize\n",
    "    X = np.zeros((N,N))\n",
    "    AX1_b = 0.0\n",
    "    \n",
    "    feasibility1 = [] # norm(A1(X)-b1)/norm(b1)\n",
    "    feasibility2 = [] # dist(X, \\mathcal{K})\n",
    "    objective    = [] # f(x)\n",
    "    cur_iter    = []   \n",
    "    t    = []         # for time tracking\n",
    "    \n",
    "    iter_track = np.unique(np.ceil(np.power(2, np.linspace(0,20,50))))\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for iteration in range(1, maxit+1):\n",
    "        \n",
    "        # Update Step Size\n",
    "        gamma = ??\n",
    "        \n",
    "        # Update beta\n",
    "        beta_ = ??\n",
    "        \n",
    "        # Write down the vk to use in the lmo (eigenvalue routine)\n",
    "        vk = ??? \n",
    "        vk = 0.5*(vk + vk.T)\n",
    "        \n",
    "        # Linear minimization oracle\n",
    "        q, u = eigsh(???, k=1, tol=1e-16, which='SA')\n",
    "        \n",
    "        if ???:\n",
    "            u = ??\n",
    "        else:\n",
    "            u = sqrt(kappa)*u\n",
    "        \n",
    "        X_bar = np.outer(u,u)\n",
    "        \n",
    "        # Obtain A*Xbar - b\n",
    "        AX_bar_b = A1(X_bar)-b\n",
    "        \n",
    "        # Update A*X - b\n",
    "        AX1_b = (1.0-gamma)*AX1_b + gamma*(AX_bar_b)\n",
    "        \n",
    "        # Update X\n",
    "        X = ???\n",
    "                \n",
    "        if any(iteration == iter_track) or iteration==maxit:\n",
    "            feasibility1.append(np.linalg.norm(AX1_b)/N)\n",
    "            feasibility2.append(np.linalg.norm(np.minimum(X,0), ord='fro'))\n",
    "            objective.append(np.sum(C.flatten()*X.flatten()))\n",
    "            cur_iter.append(iteration)\n",
    "            t.append(time.time()-start)\n",
    "            print('{:03d} | {:.4e}| {:.4e}| {:.4e}|'.format(iteration, feasibility1[-1], feasibility2[-1],objective[-1]))\n",
    "            \n",
    "    return X, feasibility1, feasibility2, objective, cur_iter, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run HCGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_HCGM, f1_HCGM, f2_HCGM, obj_HCGM, iter_HCGM, time_HCGM = HCGM(10, int(5000), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 2. Vu-Condat\n",
    "\n",
    "**Remarks:** \n",
    "\n",
    "- A similar observation applies tor the Vu-Condat algorithm: it is possible to use different dual step sizes $\\{ \\sigma_1 , \\sigma_2, \\ldots \\}$. In our case, we advise you to **multiply the step-size for $y_3$ by $10^4$** to obtain a better practical convergence. (You can directly use the tuned stepsizes for PDHG.)\n",
    "\n",
    "- In this part, you will need the projection operator of the nuclear norm ball. So, you can use what you implemented in part1 of the homework as the projection operator in the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PDHG(kappa=10, maxit=np.int(1e3), beta0=1):\n",
    "    # Initialize\n",
    "    X = np.zeros((N,N))\n",
    "    Xprev = X\n",
    "    AX1_b = 0.0\n",
    "    \n",
    "    y1 = A1(X)\n",
    "    y2 = A2(X)\n",
    "    y3 = X\n",
    "    \n",
    "    normC = np.linalg.norm(C,'fro')\n",
    "    \n",
    "    \n",
    "    feasibility1 = [] # norm(A1(X)-b1)/norm(b1)\n",
    "    feasibility2 = [] # dist(X, \\mathcal{K})\n",
    "    objective    = [] # f(x)\n",
    "    cur_iter    = [] \n",
    "    t    = [] \n",
    "    \n",
    "    L = 1e2      \n",
    "    \n",
    "    iter_track = np.unique(np.ceil(np.power(2, np.linspace(0,20,50))))\n",
    "    \n",
    "    start = time.time()\n",
    "    # Primal and dual step sizes\n",
    "    tau = 1/L # primal stepsize\n",
    "    sigma = 1/(L**2*tau) # dual step size\n",
    "    sigma2 = sigma*1e4   # different dual step size to use for the update of y3 for better practical performance.\n",
    "\n",
    "    \n",
    "    for iteration in range(1, maxit+1):\n",
    "        \n",
    "        # Primal variable update\n",
    "        Xprev = X # store the previous iterate for reflection\n",
    "        X = ???\n",
    "        \n",
    "        # Dual variable updates\n",
    "        Xhat = ??? # the point at which the dual gradient is calculated\n",
    "        y1 = ???\n",
    "        y2 = ???\n",
    "        y3 = ???\n",
    "                \n",
    "        # Update A*X - b\n",
    "        AX1_b = A1(X)-b\n",
    "        \n",
    "        if any(iteration == iter_track) or iteration==maxit:\n",
    "            feasibility1.append(np.linalg.norm(AX1_b)/N)\n",
    "            feasibility2.append(np.linalg.norm(np.minimum(X,0), ord='fro'))\n",
    "            objective.append(np.sum(C.flatten()*X.flatten()))\n",
    "            cur_iter.append(iteration)\n",
    "            t.append(time.time()-start)\n",
    "            print('{:03d} | {:.4e}| {:.4e}| {:.4e}|'.format(iteration, feasibility1[-1], feasibility2[-1],objective[-1]))\n",
    "            \n",
    "    return X, feasibility1, feasibility2, objective, cur_iter, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Vu-Condat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PDHG, f1_PDHG, f2_PDHG, obj_PDHG, iter_PDHG, time_PDHG = PDHG(10, np.int(1000), 1, opt_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = (iter_PDHG, iter_HCGM)\n",
    "times = (time_PDHG, time_HCGM)\n",
    "feas1 = (f1_PDHG, f1_HCGM)\n",
    "feas2 = (f2_PDHG, f2_HCGM)\n",
    "obj   = (obj_PDHG, obj_HCGM)\n",
    "plot_comp(times, feas1,feas2, obj, opt_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rounding: Get the assignments from the result of the SDP\n",
    "Getting the assignments requires going back to the $10$ dimensional space discussed before, and using the coordinates multiplied with the obtained matrix to construct a \"denoised\" version of the data points. This allows then to find the clusters from these $10$ dimensional data. See [3] for more details. Our implementation is the python reimplementation of their matlab code which can be found on [github](https://github.com/solevillar/kmeans_sdp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_HCGM, assign_HCGM = sdp_rounding(X_HCGM,10, digits)\n",
    "center_PDHG, assign_PDHG = sdp_rounding(X_PDHG,10, digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means value: HCGM & Vu-Condat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means_before = value_kmeans(digits, labels-1) # k_means value with true labels\n",
    "k_means_after_HCGM  = value_kmeans(digits, assign_HCGM) # k_means value with assigned lables\n",
    "k_means_after_PDHG  = value_kmeans(digits, assign_PDHG) # k_means value with assigned lables\n",
    "\n",
    "\n",
    "print('k-means value initial: {:.4f}'.format(k_means_before))\n",
    "print('k-means value for HCGM: {:.4f}'.format(k_means_after_HCGM))\n",
    "print('k-means value for Vu-Condat: {:.4f}'.format(k_means_after_PDHG))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  k-means value: Lloyd's algorithm\n",
    "\n",
    "Run the Lloyd's algorithm directly on the input digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers_Lloyd, classifications_Lloyd, k_means_Lloyd = kmeans(digits.T, 10)#k_means value with Lloyds k-means algorithm\n",
    "\n",
    "print('k-means value for Lloyd''s algorithm: {:.4f}'.format(k_means_Lloyd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\triangleright$ What are the final objective values? Are they below the optimal value provided to you? If yes, explain the reason. Answer in the box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\triangleright$ Using the function `value_kmeans`, compute and report the $k$-means value before and after running both algorithms. \n",
    "\n",
    "$\\triangleright$ Run the function `kmeans` a few times and report the $k$-means value obtained by Llyod's algorithm. Compare it with the ones obtained by rounding the solution of convex methods `HCGM` and `PDHG`. Comment on the result. (Write in the box below)\n",
    "\n",
    "(<span style=\"font-variant:small-caps;\"> Hint: </span> Note that when $\\mathcal{X}$ is as given in (Problem 1), $\\kappa u u^\\top \\in \\text{lmo}_{\\mathcal{X}}(X)$, where $u$ is the eigenvector corresponding to the smallest eigenvalue of $X$.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPTIONAL: Additional results for clustering fMNIST Data\n",
    "\n",
    "### Misclassification rates: HCGM & Vu-Condat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Misclassification rate for HCGM: {:.4f}'.format(misclassification_rate(assign_HCGM, labels)))\n",
    "print('Misclassification rate for Vu-Condat: {:.4f}'.format(misclassification_rate(assign_PDHG, labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize samples and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_samples(assignment, images, labels):\n",
    "    assignment=assignment.astype(int)\n",
    "    classes = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "    labels = labels-1\n",
    "    rand_samp = np.random.randint(0,1000,25)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    for i,samp in enumerate(rand_samp):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.imshow(1-np.reshape(images[samp],[28,28]), cmap=plt.cm.gray)\n",
    "        plt.title('Pred. {0}\\n Orig. {1}'.format(classes[assignment[samp].item()],classes[labels[samp].item()]))\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_samples(assign_HCGM, images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_samples(assign_PDHG, images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c30f2af5f468e7f5b45bcc30fca5f4886c90d54777aed916ed5f6294dfb24bf2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
